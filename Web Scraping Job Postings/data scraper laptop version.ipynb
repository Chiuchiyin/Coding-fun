{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db150f8e",
   "metadata": {},
   "source": [
    "Cleaner version of the notebook that I run on laptop to scape data faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbe9f2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "from time import time, sleep\n",
    "import random\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from IPython.display import clear_output\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "505f6636",
   "metadata": {},
   "outputs": [],
   "source": [
    "#captcha alert settings\n",
    "import winsound\n",
    "duration = 5000  # milliseconds\n",
    "freq = 440  # Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddb60230",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'jobs1.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbf79594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file successfully opened\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    df_more = pd.read_csv(file , sep='\\t', encoding='utf-8', index_col=0)\n",
    "    print('file successfully opened')\n",
    "except:\n",
    "    print('file not found, creating new file')\n",
    "    df_more = pd.DataFrame(columns=[\"Title\",\"Location\",\"Company\",\"Salary\", \"Synopsis\"])\n",
    "    df_more.to_csv(file, sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b83f71d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>Company</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Synopsis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Junior Data Scientist</td>\n",
       "      <td>Austin, TX 78735</td>\n",
       "      <td>Kestra Financial</td>\n",
       "      <td>$84.2K - $107K a year</td>\n",
       "      <td>Create complex data pipelines, while learning ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist I (22-0623)</td>\n",
       "      <td>Austin, TX 78741 (Pleasant Valley area)</td>\n",
       "      <td>Office of the Attorney General of Texas</td>\n",
       "      <td>None</td>\n",
       "      <td>Develops data quality measures, analyzes data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Austin, TX 78701 (Downtown area)</td>\n",
       "      <td>Zilliant</td>\n",
       "      <td>$110K - $140K a year</td>\n",
       "      <td>Experience implementing data science routines ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Hybrid remote in Austin, TX</td>\n",
       "      <td>YETI</td>\n",
       "      <td>$103K - $131K a year</td>\n",
       "      <td>Experience with data visualization tools (e.g....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Junior Data Scientist</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>ANALYTOS</td>\n",
       "      <td>$67.6K - $85.6K a year</td>\n",
       "      <td>Designs and drives the creation of new standar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13043</th>\n",
       "      <td>Customer Engineer (ML/AI/Kubernetes)</td>\n",
       "      <td>Remote in Chicago, IL</td>\n",
       "      <td>Pachyderm</td>\n",
       "      <td>$113K - $143K a year</td>\n",
       "      <td>Pachyderm helps customers get their ML and AI ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13044</th>\n",
       "      <td>Senior Machine Learning Architect</td>\n",
       "      <td>Chicago, IL+1 location</td>\n",
       "      <td>Credera Experienced Hiring Job Board</td>\n",
       "      <td>$184K - $233K a year</td>\n",
       "      <td>Our data capabilities help our clients gain co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13045</th>\n",
       "      <td>Staff ML Engineer - Remote</td>\n",
       "      <td>Remote in Chicago, IL</td>\n",
       "      <td>Clari</td>\n",
       "      <td>$124K - $157K a year</td>\n",
       "      <td>A passion for answering questions by analyzing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13046</th>\n",
       "      <td>Lead Data Engineer - Snowflake</td>\n",
       "      <td>Remote in Rosemont, IL</td>\n",
       "      <td>Merchants Fleet</td>\n",
       "      <td>$118K - $149K a year</td>\n",
       "      <td>Experience analyzing data for data quality and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13047</th>\n",
       "      <td>Head Algorithmic Trader - Commodities</td>\n",
       "      <td>Chicago, IL 60661 (Near West Side area)</td>\n",
       "      <td>Eagle Seven</td>\n",
       "      <td>None</td>\n",
       "      <td>The Head Trader will be responsible for levera...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13048 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Title  \\\n",
       "0                      Junior Data Scientist   \n",
       "1                 Data Scientist I (22-0623)   \n",
       "2                             Data Scientist   \n",
       "3                             Data Scientist   \n",
       "4                      Junior Data Scientist   \n",
       "...                                      ...   \n",
       "13043   Customer Engineer (ML/AI/Kubernetes)   \n",
       "13044      Senior Machine Learning Architect   \n",
       "13045             Staff ML Engineer - Remote   \n",
       "13046         Lead Data Engineer - Snowflake   \n",
       "13047  Head Algorithmic Trader - Commodities   \n",
       "\n",
       "                                      Location  \\\n",
       "0                             Austin, TX 78735   \n",
       "1      Austin, TX 78741 (Pleasant Valley area)   \n",
       "2             Austin, TX 78701 (Downtown area)   \n",
       "3                  Hybrid remote in Austin, TX   \n",
       "4                                   Austin, TX   \n",
       "...                                        ...   \n",
       "13043                    Remote in Chicago, IL   \n",
       "13044                   Chicago, IL+1 location   \n",
       "13045                    Remote in Chicago, IL   \n",
       "13046                   Remote in Rosemont, IL   \n",
       "13047  Chicago, IL 60661 (Near West Side area)   \n",
       "\n",
       "                                       Company                   Salary  \\\n",
       "0                             Kestra Financial    $84.2K - $107K a year   \n",
       "1      Office of the Attorney General of Texas                     None   \n",
       "2                                     Zilliant     $110K - $140K a year   \n",
       "3                                         YETI     $103K - $131K a year   \n",
       "4                                     ANALYTOS   $67.6K - $85.6K a year   \n",
       "...                                        ...                      ...   \n",
       "13043                                Pachyderm     $113K - $143K a year   \n",
       "13044     Credera Experienced Hiring Job Board     $184K - $233K a year   \n",
       "13045                                    Clari     $124K - $157K a year   \n",
       "13046                          Merchants Fleet     $118K - $149K a year   \n",
       "13047                              Eagle Seven                     None   \n",
       "\n",
       "                                                Synopsis  \n",
       "0      Create complex data pipelines, while learning ...  \n",
       "1      Develops data quality measures, analyzes data ...  \n",
       "2      Experience implementing data science routines ...  \n",
       "3      Experience with data visualization tools (e.g....  \n",
       "4      Designs and drives the creation of new standar...  \n",
       "...                                                  ...  \n",
       "13043  Pachyderm helps customers get their ML and AI ...  \n",
       "13044  Our data capabilities help our clients gain co...  \n",
       "13045  A passion for answering questions by analyzing...  \n",
       "13046  Experience analyzing data for data quality and...  \n",
       "13047  The Head Trader will be responsible for levera...  \n",
       "\n",
       "[13048 rows x 5 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c221d33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_list = [\n",
    "# Firefox 77 Mac\n",
    "{\n",
    "\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:77.0) Gecko/20100101 Firefox/77.0\",\n",
    "\"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "\"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "\"Referer\": \"https://www.google.com/\",\n",
    "\"DNT\": \"1\",\n",
    "\"Connection\": \"keep-alive\",\n",
    "\"Upgrade-Insecure-Requests\": \"1\"\n",
    "},\n",
    "# Firefox 77 Windows\n",
    "{\n",
    "\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:77.0) Gecko/20100101 Firefox/77.0\",\n",
    "\"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "\"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "\"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "\"Referer\": \"https://www.google.com/\",\n",
    "\"DNT\": \"1\",\n",
    "\"Connection\": \"keep-alive\",\n",
    "\"Upgrade-Insecure-Requests\": \"1\"\n",
    "},\n",
    "# Chrome 83 Mac\n",
    "{\n",
    "\"Connection\": \"keep-alive\",\n",
    "\"DNT\": \"1\",\n",
    "\"Upgrade-Insecure-Requests\": \"1\",\n",
    "\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\",\n",
    "\"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "\"Sec-Fetch-Site\": \"none\",\n",
    "\"Sec-Fetch-Mode\": \"navigate\",\n",
    "\"Sec-Fetch-Dest\": \"document\",\n",
    "\"Referer\": \"https://www.google.com/\",\n",
    "\"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "\"Accept-Language\": \"en-GB,en-US;q=0.9,en;q=0.8\"\n",
    "},\n",
    "# Chrome 83 Windows \n",
    "{\n",
    "\"Connection\": \"keep-alive\",\n",
    "\"Upgrade-Insecure-Requests\": \"1\",\n",
    "\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\",\n",
    "\"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "\"Sec-Fetch-Site\": \"same-origin\",\n",
    "\"Sec-Fetch-Mode\": \"navigate\",\n",
    "\"Sec-Fetch-User\": \"?1\",\n",
    "\"Sec-Fetch-Dest\": \"document\",\n",
    "\"Referer\": \"https://www.google.com/\",\n",
    "\"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "\"Accept-Language\": \"en-US,en;q=0.9\"\n",
    "}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c5cc738",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_template = \"http://www.indeed.com/jobs?q=data+scientist+%2420%2C000&l={}&start={}\"\n",
    "max_results_per_city = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "438187ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55251, 5) data saved, last visited city is  Miami , continuing....\n",
      "2827 loops done, proceed to sleep, current city is  Miami\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'jobs1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_33672/2256940770.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mcount\u001b[0m\u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'loops done, proceed to sleep, current city is '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0mdf_more\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#write every page so i don't lose my data again\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m         \u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m#I don't want too see long line of the process, clear_output will clear previous lines\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3464\u001b[0m         )\n\u001b[0;32m   3465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3466\u001b[1;33m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[0;32m   3467\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3468\u001b[0m             \u001b[0mline_terminator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mline_terminator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1103\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         )\n\u001b[1;32m-> 1105\u001b[1;33m         \u001b[0mcsv_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1107\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    235\u001b[0m         \"\"\"\n\u001b[0;32m    236\u001b[0m         \u001b[1;31m# apply compression and byte/text conversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m         with get_handle(\n\u001b[0m\u001b[0;32m    238\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: 'jobs1.csv'"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "count = 0\n",
    "results = []\n",
    "\n",
    "for city in set(['New+York', 'Chicago', 'San+Francisco', 'Austin', 'Seattle', \n",
    "                 'Los+Angeles', 'Philadelphia', 'Atlanta', 'Dallas', 'Pittsburgh', \n",
    "                 'Portland', 'Phoenix', 'Denver', 'Houston', 'Miami']):\n",
    "    for start in range(0, max_results_per_city, 10):\n",
    "        # Grab the results from the request (as above)\n",
    "        # Append to the full set of results\n",
    "        url = url_template.format(city, start)\n",
    "        headers = random.choice(headers_list)\n",
    "        r = requests.Session()\n",
    "        r.headers = headers\n",
    "        html = r.get(url)\n",
    "        #html = requests.get(url)\n",
    "        soup = BeautifulSoup(html.content, 'html.parser', from_encoding=\"utf-8\")\n",
    "        #captcha detector\n",
    "        if 'hCaptcha solve page' in soup.get_text():\n",
    "            print('captcha detected')\n",
    "            winsound.Beep(freq, duration) #sound alert, might get irritating, probably won't work on mac\n",
    "            input(\"Press Enter to continue...\")#this would pause the code until i press enter\n",
    "        for each in soup.find_all(class_= \"result\" ):\n",
    "            try: \n",
    "                title = each.find(class_='jobTitle').text.replace('\\n', '').replace('new', '')\n",
    "            except:\n",
    "                title = 'None'\n",
    "            try:\n",
    "                location = each.find('div', {'class':\"companyLocation\" }).text.replace('\\n', '')\n",
    "            except:\n",
    "                location = 'None'\n",
    "            try: \n",
    "                company = each.find(class_='companyName').text.replace('\\n', '')\n",
    "            except:\n",
    "                company = 'None'\n",
    "            try:\n",
    "                salary = each.find('span', {'class':'estimated-salary'}).text.replace('\\n', '').replace('Estimated', '')\n",
    "            except:\n",
    "                salary = 'None'\n",
    "            synopsis = each.find('div', {'class':'job-snippet'}).text.replace('\\n', '')\n",
    "            df_more = df_more.append({'Title':title, 'Location':location, 'Company':company, 'Salary':salary, 'Synopsis':synopsis}, ignore_index=True)\n",
    "            i += 1\n",
    "            if i % 1000 == 0:\n",
    "                print('You have ' + str(i) + ' results. ' + str(df_more.dropna().drop_duplicates().shape[0]) + \" of these aren't rubbish.\")\n",
    "        #count the number of pages scraped and print, so I know how far the code is running\n",
    "        count+= 1\n",
    "        print(count, 'loops done, proceed to sleep, current city is ', city)\n",
    "        df_more.to_csv(file, sep='\\t', encoding='utf-8')#write every page so i don't lose my data again\n",
    "        sleep(random.randint(7,20))\n",
    "        #I don't want too see long line of the process, clear_output will clear previous lines\n",
    "        clear_output(wait=True)\n",
    "        print(df_more.shape, 'data saved, last visited city is ', city, ', continuing....')\n",
    "    print('moving to next city')\n",
    "    sleep(random.randint(30,60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03155ea4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
