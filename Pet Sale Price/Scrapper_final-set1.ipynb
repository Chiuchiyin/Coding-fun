{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b00e099",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-06T08:32:36.178686Z",
     "start_time": "2022-06-06T08:32:35.576325Z"
    }
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "from time import time, sleep\n",
    "import random\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support.expected_conditions import visibility_of_element_located\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import winsound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0dd755",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-06T08:32:36.832602Z",
     "start_time": "2022-06-06T08:32:36.827722Z"
    }
   },
   "outputs": [],
   "source": [
    "url_template = \"https://www.pets4homes.co.uk/{}/{}/local/{}/\"\n",
    "url_template_pages = \"https://www.pets4homes.co.uk/{}/{}/local/{}/page-{}/\"\n",
    "listing_type = ['sale','adoption','stud']\n",
    "pet_type = ['dogs','cats','reptiles','rodents','rabbits','horses','invertebrates','livestock', 'poultry', 'birds','fish']\n",
    "#cities = ['aberdeen','ayr_st-ives', 'barnsley_wimborne','basingstoke','bath','bedfordshire','birmingham','blackpool_plymouth',\n",
    "#         'bolton_appleby-in-westmorland', 'bournemouth', 'bradford_holsworthy', 'bridgend_brechin', 'brighton',\n",
    "#          'bristol_south-west-england', 'cambridge_gloucester', 'cardiff-county', 'carlisle', 'chelmsford', 'cheltenham',\n",
    "#         'chester']\n",
    "cities = ['aberdeen', 'basingstoke',\n",
    "          'bournemouth', 'bridgend_brechin',\n",
    "          'bristol_south-west-england', 'chelmsford',\n",
    "         'chester']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fe8b2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-06T08:32:37.614676Z",
     "start_time": "2022-06-06T08:32:37.609796Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_captcha (soup):\n",
    "    if 'I am human' in soup.get_text():\n",
    "            print('captcha detected')\n",
    "            duration = 5000  # milliseconds\n",
    "            freq = 440  # Hz\n",
    "            winsound.Beep(freq, duration) #sound alert, might get irritating, probably won't work on mac\n",
    "            input(\"Press Enter to continue...\")#this would pause the code until i press enter\n",
    "    else:\n",
    "        print('no captcha detected, proceed to scrape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a12c233",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-06T08:32:38.446960Z",
     "start_time": "2022-06-06T08:32:38.441662Z"
    }
   },
   "outputs": [],
   "source": [
    "def scrape_header(soup):\n",
    "    df = pd.DataFrame(columns=[\"Title\",\"Description\",\"photo_link\",\"category\", \"price\", \"url\",\n",
    "                                              \"seller_type\", \"seller_name\", 'listing_type','pet_type'])\n",
    "    for each in soup.find_all(\"script\", type=\"application/ld+json\"):\n",
    "        entries = each.get_text()\n",
    "        split = entries.split(\"\\n\")\n",
    "        Title = split[3].replace('\"name\": \"', '').replace('\",', '')\n",
    "        Description = split[4].replace('\"description\": \"', '').replace('\",', '').replace('\\n,', '')\n",
    "        Photo = split[5].replace('\"image\": \"', '').replace('\",', '')\n",
    "        Category = split[7].replace('\"category\": \"', '').replace('\",', '')\n",
    "        Price = split[11].replace('\"price\": \"', '').replace('\",', '')\n",
    "        url =split[13].replace('\"url\": \"', '').replace('\",', '')\n",
    "        seller_type = split[15].replace('\"type\": \"', '').replace('\",', '')\n",
    "        seller_name = split[16].replace('\"name\":  \"', '').replace('\"', '')\n",
    "        df = df.append({\"Title\":Title,\"Description\":Description,\"photo_link\":Photo,\"category\":Category, \n",
    "                                      \"price\":Price,\"url\":url, \"seller_type\":seller_type, \"seller_name\":seller_name,\n",
    "                                      \"listing_type\":listing, \"pet_type\": pets},ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130b2335",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-06T08:32:39.183930Z",
     "start_time": "2022-06-06T08:32:39.178075Z"
    }
   },
   "outputs": [],
   "source": [
    "def scrape_content(soup):\n",
    "    df1 = pd.DataFrame(columns=[\"Title\",\"price\",\"species\", \"age\", \"gender\", \"description\",\n",
    "                                \"seller_name\", \"seller_location\", \"seller_type\", \"listing_type\",\"pet_type\"])\n",
    "    for each in soup.find_all(\"div\", class_=\"Xm lj\"):\n",
    "        entries = each.get_text()\n",
    "        title = each.find('h2', {'class':'gn'}).text\n",
    "        price = each.find('span', {'class':'hn'}).text\n",
    "        info = each.find_all('span', {'class':'nv'})\n",
    "        species = info[0].text\n",
    "        try:\n",
    "            age = info[1].text.replace('Age:  ', '')\n",
    "        except:\n",
    "            age = 'unknown'\n",
    "        try:\n",
    "            gender = info[2].text\n",
    "        except:\n",
    "            gender = 'unknown'\n",
    "        description = each.find('span', {'class':'in'}).text\n",
    "        seller_name = each.find('span', {'class':'on'}).text\n",
    "        seller_location = each.find('span', {'class':'gs'}).text\n",
    "        seller_type = each.find('div', {'class':'ef Pl ff'}).text\n",
    "        df1 = df1.append({\"Title\":title, \"price\":price, \"species\":species, \"age\":age, \"gender\":gender,\n",
    "                              \"description\":description, \"seller_name\":seller_name, \"seller_location\": seller_location,\n",
    "                              \"seller_type\":seller_type, \"listing_type\":listing, \"pet_type\": pets},ignore_index=True)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71dd351",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-06-06T08:32:39.949Z"
    }
   },
   "outputs": [],
   "source": [
    "completed = []\n",
    "for city in set (cities):\n",
    "    for pets in set(pet_type):\n",
    "        for listing in set(listing_type):\n",
    "            page = 1\n",
    "            while page > 0:\n",
    "                clear_output(wait=True)\n",
    "                file = 'data_by_cities/header_'+str(city)+'.csv'\n",
    "                file1 = 'data_by_cities/body_'+str(city)+'.csv'\n",
    "                try: \n",
    "                    df_header = pd.read_csv(file , sep='\\t', encoding='utf-8', index_col=0)\n",
    "                    print('file successfully opened')\n",
    "                except:\n",
    "                    print('file not found, creating new file')\n",
    "                    df_header = pd.DataFrame(columns=[\"Title\",\"Description\",\"photo_link\",\"category\", \"price\", \"url\",\n",
    "                                              \"seller_type\", \"seller_name\", 'listing_type','pet_type'])\n",
    "                    df_header.to_csv(file, sep='\\t', encoding='utf-8')\n",
    "                try: \n",
    "                    df_body = pd.read_csv(file1 , sep='\\t', encoding='utf-8', index_col=0)\n",
    "                    print('file successfully opened')\n",
    "                except:\n",
    "                    print('file not found, creating new file')\n",
    "                    df_body = pd.DataFrame(columns=[\"Title\",\"price\",\"species\", \"age\", \"gender\", \"description\",\n",
    "                                                    \"seller_name\", \"seller_location\", \"seller_type\" , \n",
    "                                                    \"listing_type\",\"pet_type\"])\n",
    "                    df_body.to_csv(file1, sep='\\t', encoding='utf-8')\n",
    "                headersize = df_header.shape\n",
    "                bodysize = df_body.shape\n",
    "                if page == 1:\n",
    "                    url = url_template.format(listing, pets, city)\n",
    "                else:\n",
    "                    url = url_template_pages.format(listing, pets, city, page)\n",
    "                options = webdriver.ChromeOptions()\n",
    "                browser = webdriver.Chrome(ChromeDriverManager().install(),options = options)\n",
    "                browser.get(url)\n",
    "                title = (\n",
    "                    WebDriverWait(driver=browser, timeout=15)\n",
    "                    .until(visibility_of_element_located((By.CSS_SELECTOR, \"h1\"))).text\n",
    "                )\n",
    "                content = browser.page_source\n",
    "                print(city,pets,listing,page)\n",
    "                soup = BeautifulSoup(content, 'lxml')\n",
    "                check_captcha(soup)\n",
    "                if 'We found 0' in soup.get_text():\n",
    "                    print('no result found')\n",
    "                    break\n",
    "                df_header = df_header.append(scrape_header(soup))\n",
    "                df_body = df_body.append(scrape_content(soup))\n",
    "                #drop duplicates\n",
    "                df_header = df_header[~df_header.duplicated(subset=['Title','Description', 'photo_link','category','url','seller_type','listing_type','pet_type'])].reset_index(drop=True)\n",
    "                df_header.to_csv(file, sep='\\t', encoding='utf-8')\n",
    "                df_body = df_body[~df_body.duplicated()].reset_index(drop=True)\n",
    "                df_body.to_csv(file1, sep='\\t', encoding='utf-8')\n",
    "                if df_body.shape == bodysize:\n",
    "                    if df_header.shape == headersize:\n",
    "                        print('no new data')\n",
    "                    else:\n",
    "                        winsound.Beep(600, 1000)\n",
    "                        input('html tags is likely changed, please kill program')\n",
    "                next_page = soup.find('a', {'class':'Gb Xg Kb'})\n",
    "                if next_page == None:\n",
    "                    print('next page not found')\n",
    "                    break\n",
    "                else:\n",
    "                    print ('next page found')\n",
    "                    print('going to next page')\n",
    "                    sleep(random.randint(10,15))\n",
    "                    page +=1\n",
    "            print('going to next listing type')\n",
    "            sleep(random.randint(10,15))\n",
    "        print('going to next pet type')\n",
    "    completed.append(city)\n",
    "    print('going to next city')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46b3a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab527fd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
