{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b00e099",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-06T08:32:36.178686Z",
     "start_time": "2022-06-06T08:32:35.576325Z"
    }
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "from time import time, sleep\n",
    "import random\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support.expected_conditions import visibility_of_element_located\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import winsound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e0dd755",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-06T08:32:36.832602Z",
     "start_time": "2022-06-06T08:32:36.827722Z"
    }
   },
   "outputs": [],
   "source": [
    "url_template = \"https://www.pets4homes.co.uk/{}/{}/local/{}/\"\n",
    "url_template_pages = \"https://www.pets4homes.co.uk/{}/{}/local/{}/page-{}/\"\n",
    "listing_type = ['sale','adoption','stud']\n",
    "pet_type = ['dogs','cats','reptiles','rodents','rabbits','horses','invertebrates','livestock', 'poultry', 'birds','fish']\n",
    "#cities = ['aberdeen','ayr_st-ives', 'barnsley_wimborne','basingstoke','bath','bedfordshire','birmingham','blackpool_plymouth',\n",
    "#         'bolton_appleby-in-westmorland', 'bournemouth', 'bradford_holsworthy', 'bridgend_brechin', 'brighton',\n",
    "#          'bristol_south-west-england', 'cambridge_gloucester', 'cardiff-county', 'carlisle', 'chelmsford', 'cheltenham',\n",
    "#         'chester']\n",
    "cities = ['aberdeen', 'basingstoke',\n",
    "          'bournemouth', 'bridgend_brechin',\n",
    "          'bristol_south-west-england', 'chelmsford',\n",
    "         'chester']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62fe8b2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-06T08:32:37.614676Z",
     "start_time": "2022-06-06T08:32:37.609796Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_captcha (soup):\n",
    "    if 'I am human' in soup.get_text():\n",
    "            print('captcha detected')\n",
    "            duration = 5000  # milliseconds\n",
    "            freq = 440  # Hz\n",
    "            winsound.Beep(freq, duration) #sound alert, might get irritating, probably won't work on mac\n",
    "            input(\"Press Enter to continue...\")#this would pause the code until i press enter\n",
    "    else:\n",
    "        print('no captcha detected, proceed to scrape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a12c233",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-06T08:32:38.446960Z",
     "start_time": "2022-06-06T08:32:38.441662Z"
    }
   },
   "outputs": [],
   "source": [
    "def scrape_header(soup):\n",
    "    df = pd.DataFrame(columns=[\"Title\",\"Description\",\"photo_link\",\"category\", \"price\", \"url\",\n",
    "                                              \"seller_type\", \"seller_name\", 'listing_type','pet_type'])\n",
    "    for each in soup.find_all(\"script\", type=\"application/ld+json\"):\n",
    "        entries = each.get_text()\n",
    "        split = entries.split(\"\\n\")\n",
    "        Title = split[3].replace('\"name\": \"', '').replace('\",', '')\n",
    "        Description = split[4].replace('\"description\": \"', '').replace('\",', '').replace('\\n,', '')\n",
    "        Photo = split[5].replace('\"image\": \"', '').replace('\",', '')\n",
    "        Category = split[7].replace('\"category\": \"', '').replace('\",', '')\n",
    "        Price = split[11].replace('\"price\": \"', '').replace('\",', '')\n",
    "        url =split[13].replace('\"url\": \"', '').replace('\",', '')\n",
    "        seller_type = split[15].replace('\"type\": \"', '').replace('\",', '')\n",
    "        seller_name = split[16].replace('\"name\":  \"', '').replace('\"', '')\n",
    "        df = df.append({\"Title\":Title,\"Description\":Description,\"photo_link\":Photo,\"category\":Category, \n",
    "                                      \"price\":Price,\"url\":url, \"seller_type\":seller_type, \"seller_name\":seller_name,\n",
    "                                      \"listing_type\":listing, \"pet_type\": pets},ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "130b2335",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-06T08:32:39.183930Z",
     "start_time": "2022-06-06T08:32:39.178075Z"
    }
   },
   "outputs": [],
   "source": [
    "def scrape_content(soup):\n",
    "    df1 = pd.DataFrame(columns=[\"Title\",\"price\",\"species\", \"age\", \"gender\", \"description\",\n",
    "                                \"seller_name\", \"seller_location\", \"seller_type\", \"listing_type\",\"pet_type\"])\n",
    "    for each in soup.find_all(\"div\", class_=\"mn bj\"):\n",
    "        entries = each.get_text()\n",
    "        title = each.find('h2', {'class':'wn'}).text\n",
    "        price = each.find('span', {'class':'xn'}).text\n",
    "        info = each.find_all('span', {'class':'rv'})\n",
    "        species = info[0].text\n",
    "        try:\n",
    "            age = info[1].text.replace('Age:  ', '')\n",
    "        except:\n",
    "            age = 'unknown'\n",
    "        try:\n",
    "            gender = info[2].text\n",
    "        except:\n",
    "            gender = 'unknown'\n",
    "        description = each.find('span', {'class':'yn'}).text\n",
    "        seller_name = each.find('span', {'class':'En'}).text\n",
    "        seller_location = each.find('span', {'class':'vs'}).text\n",
    "        seller_type = each.find('div', {'class':'Le Il Me'}).text\n",
    "        df1 = df1.append({\"Title\":title, \"price\":price, \"species\":species, \"age\":age, \"gender\":gender,\n",
    "                              \"description\":description, \"seller_name\":seller_name, \"seller_location\": seller_location,\n",
    "                              \"seller_type\":seller_type, \"listing_type\":listing, \"pet_type\": pets},ignore_index=True)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f71dd351",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-06-06T08:32:39.949Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file successfully opened\n",
      "file successfully opened\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - ====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 102.0.5005\n",
      "[WDM] - Get LATEST chromedriver version for 102.0.5005 google-chrome\n",
      "[WDM] - Driver [C:\\Users\\silve\\.wdm\\drivers\\chromedriver\\win32\\102.0.5005.61\\chromedriver.exe] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bristol_south-west-england dogs sale 8\n",
      "no captcha detected, proceed to scrape\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_136968/90004977.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m                         \u001b[0mwinsound\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBeep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m600\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m                         \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'html tags is likely changed, please kill program'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m                 \u001b[0mnext_page\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'Fb jh Jb'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mnext_page\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1004\u001b[0m                 \u001b[1;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1005\u001b[0m             )\n\u001b[1;32m-> 1006\u001b[1;33m         return self._input_request(\n\u001b[0m\u001b[0;32m   1007\u001b[0m             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1008\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"shell\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1049\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m                 \u001b[1;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1051\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Interrupted by user\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid Message:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "completed = []\n",
    "for city in set (cities):\n",
    "    for pets in set(pet_type):\n",
    "        for listing in set(listing_type):\n",
    "            page = 1\n",
    "            while page > 0:\n",
    "                clear_output(wait=True)\n",
    "                file = 'data_by_cities/header_'+str(city)+'.csv'\n",
    "                file1 = 'data_by_cities/body_'+str(city)+'.csv'\n",
    "                try: \n",
    "                    df_header = pd.read_csv(file , sep='\\t', encoding='utf-8', index_col=0)\n",
    "                    print('file successfully opened')\n",
    "                except:\n",
    "                    print('file not found, creating new file')\n",
    "                    df_header = pd.DataFrame(columns=[\"Title\",\"Description\",\"photo_link\",\"category\", \"price\", \"url\",\n",
    "                                              \"seller_type\", \"seller_name\", 'listing_type','pet_type'])\n",
    "                    df_header.to_csv(file, sep='\\t', encoding='utf-8')\n",
    "                try: \n",
    "                    df_body = pd.read_csv(file1 , sep='\\t', encoding='utf-8', index_col=0)\n",
    "                    print('file successfully opened')\n",
    "                except:\n",
    "                    print('file not found, creating new file')\n",
    "                    df_body = pd.DataFrame(columns=[\"Title\",\"price\",\"species\", \"age\", \"gender\", \"description\",\n",
    "                                                    \"seller_name\", \"seller_location\", \"seller_type\" , \n",
    "                                                    \"listing_type\",\"pet_type\"])\n",
    "                    df_body.to_csv(file1, sep='\\t', encoding='utf-8')\n",
    "                headersize = df_header.shape\n",
    "                bodysize = df_body.shape\n",
    "                if page == 1:\n",
    "                    url = url_template.format(listing, pets, city)\n",
    "                else:\n",
    "                    url = url_template_pages.format(listing, pets, city, page)\n",
    "                options = webdriver.ChromeOptions()\n",
    "                browser = webdriver.Chrome(ChromeDriverManager().install(),options = options)\n",
    "                browser.get(url)\n",
    "                title = (\n",
    "                    WebDriverWait(driver=browser, timeout=15)\n",
    "                    .until(visibility_of_element_located((By.CSS_SELECTOR, \"h1\"))).text\n",
    "                )\n",
    "                content = browser.page_source\n",
    "                print(city,pets,listing,page)\n",
    "                soup = BeautifulSoup(content, 'lxml')\n",
    "                check_captcha(soup)\n",
    "                if 'We found 0' in soup.get_text():\n",
    "                    print('no result found')\n",
    "                    break\n",
    "                df_header = df_header.append(scrape_header(soup))\n",
    "                df_body = df_body.append(scrape_content(soup))\n",
    "                #drop duplicates\n",
    "                df_header = df_header[~df_header.duplicated(subset=['Title','Description', 'photo_link','category','url','seller_type','listing_type','pet_type'])].reset_index(drop=True)\n",
    "                df_header.to_csv(file, sep='\\t', encoding='utf-8')\n",
    "                df_body = df_body[~df_body.duplicated()].reset_index(drop=True)\n",
    "                df_body.to_csv(file1, sep='\\t', encoding='utf-8')\n",
    "                if df_body.shape == bodysize:\n",
    "                    if df_header.shape == headersize:\n",
    "                        print('no new data')\n",
    "                    else:\n",
    "                        winsound.Beep(600, 1000)\n",
    "                        input('html tags is likely changed, please kill program')\n",
    "                next_page = soup.find('a', {'class':'Fb jh Jb'})\n",
    "                if next_page == None:\n",
    "                    print('next page not found')\n",
    "                    break\n",
    "                else:\n",
    "                    print ('next page found')\n",
    "                    print('going to next page')\n",
    "                    sleep(random.randint(10,15))\n",
    "                    page +=1\n",
    "            print('going to next listing type')\n",
    "            sleep(random.randint(10,15))\n",
    "        print('going to next pet type')\n",
    "    completed.append(city)\n",
    "    print('going to next city')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a46b3a22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brighton',\n",
       " 'birmingham',\n",
       " 'carlisle',\n",
       " 'cheltenham',\n",
       " 'cambridge_gloucester',\n",
       " 'bedfordshire',\n",
       " 'cardiff-county',\n",
       " 'ayr_st-ives',\n",
       " 'blackpool_plymouth']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b3adf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
